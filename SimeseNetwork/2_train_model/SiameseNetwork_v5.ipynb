{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
    "from tensorflow.keras.layers import Activation, BatchNormalization, Flatten\n",
    "import tensorflow.compat.v2 as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import save_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#     try:\n",
    "#         tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "#         logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "#         print(len(gpus), \"Physical GPUs\", len(logical_gpus), 'Logical GPU')\n",
    "#     except RuntimeError as e:\n",
    "#         print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setup paths\n",
    "POS_PATH = os.path.join('data', 'positive')\n",
    "NEG_PATH = os.path.join('data', 'negative')\n",
    "ANC_PATH = os.path.join('data', 'anchor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "NEW_POS_PATH = os.path.join('data4', 'positive')\n",
    "NEW_NEG_PATH = os.path.join('data4', 'negative')\n",
    "NEW_ANC_PATH = os.path.join('data4', 'anchor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = tf.data.Dataset.list_files(NEW_ANC_PATH+'/*.jpg').take(600)\n",
    "positive = tf.data.Dataset.list_files(NEW_POS_PATH+'/*.jpg').take(600)\n",
    "negative = tf.data.Dataset.list_files(NEW_NEG_PATH+'/*.jpg').take(600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_test = anchor.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'data4/anchor/baker_anchor2_186.jpg'\n"
     ]
    }
   ],
   "source": [
    "print(dir_test.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path):\n",
    "    \n",
    "    # Read in image from file path\n",
    "    byte_img = tf.io.read_file(file_path)\n",
    "    \n",
    "    # Load in the image\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    \n",
    "    # Preprocessing step - resizing the image to be 100x100x3\n",
    "    img = tf.image.resize(img, (100,100))\n",
    "    \n",
    "    # Scale image to be between 0 and 1\n",
    "    img = img / 255.0\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = tf.data.Dataset.zip((anchor, positive, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
    "negatives = tf.data.Dataset.zip((anchor, negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n",
    "data = positives.concatenate(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  preprocess_twin(input_img, validation_img, label):\n",
    "    return (preprocess(input_img), preprocess(validation_img), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataloade pipeline\n",
    "data = data.map(preprocess_twin)\n",
    "data = data.cache()\n",
    "data = data.shuffle(buffer_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training partition\n",
    "train_data = data.take(round(len(data)*.7))\n",
    "train_data = train_data.batch(16)\n",
    "train_data = train_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = train_data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing partition\n",
    "test_data = data.skip(round(len(data)*.7))\n",
    "test_Data = test_data.take(round(len(data)*.3))\n",
    "test_data = test_data.batch(16)\n",
    "test_data = test_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding():\n",
    "    \n",
    "    embedding = Sequential()\n",
    "    \n",
    "    # First block\n",
    "    embedding.add(Conv2D(filters=4,\n",
    "                         kernel_size=(5,5),\n",
    "                         padding=\"same\",\n",
    "                         input_shape=(100,100,1)))\n",
    "    embedding.add(BatchNormalization())\n",
    "    embedding.add(Activation(\"relu\"))\n",
    "    embedding.add(MaxPooling2D())\n",
    "    \n",
    "    # Second block\n",
    "    embedding.add(Conv2D(filters=4,\n",
    "                         kernel_size=(5,5),\n",
    "                         padding=\"same\",\n",
    "                         input_shape=(100,100,1)))\n",
    "    embedding.add(BatchNormalization())\n",
    "    embedding.add(Activation(\"relu\"))\n",
    "    embedding.add(MaxPooling2D())\n",
    "    \n",
    "    # Third block\n",
    "    embedding.add(Conv2D(filters=8,\n",
    "                         kernel_size=(5,5),\n",
    "                         padding=\"same\",\n",
    "                         input_shape=(100,100,1)))\n",
    "    embedding.add(BatchNormalization())\n",
    "    embedding.add(Activation(\"relu\"))\n",
    "    embedding.add(MaxPooling2D())\n",
    "    \n",
    "    # Final embedding block\n",
    "    embedding.add(Conv2D(filters=8,\n",
    "                         kernel_size=(5,5),\n",
    "                         padding=\"same\",\n",
    "                         input_shape=(100,100,1)))\n",
    "    embedding.add(BatchNormalization())\n",
    "    embedding.add(Flatten())\n",
    "    embedding.add(Dense(64, activation='sigmoid'))\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = make_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 100, 100, 4)       104       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 100, 100, 4)       16        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 100, 100, 4)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 50, 50, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 50, 50, 4)         404       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 50, 50, 4)         16        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 50, 50, 4)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 25, 25, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 25, 25, 8)         808       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 25, 25, 8)         32        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 25, 25, 8)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 12, 12, 8)         1608      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 12, 12, 8)         32        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                73792     \n",
      "=================================================================\n",
      "Total params: 76,812\n",
      "Trainable params: 76,764\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese L1 Distance class\n",
    "class L1Dist(Layer):\n",
    "    \n",
    "    # Init method - inheritance\n",
    "    def __inint__(self, **kwargs):\n",
    "        super().__init__()\n",
    "    \n",
    "    # Magic happens here - similarity caculation\n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "        return tf.math.abs(input_embedding - validation_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = L1Dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_siamese_model():\n",
    "    \n",
    "    # Anchor image input in the network\n",
    "    input_image = Input(name='input_img', shape=(100,100,1))\n",
    "    \n",
    "    # Validation image in the network\n",
    "    validation_image = Input(name='validation_img', shape=(100,100,1))\n",
    "    \n",
    "    # Combine siamese distance componets\n",
    "    siamese_layer = L1Dist()\n",
    "    siamese_layer._name = 'distance'\n",
    "    distances = siamese_layer(embedding(input_image), embedding(validation_image))\n",
    "    \n",
    "    # classification layer\n",
    "    classifier = Dense(1, activation='sigmoid')(distances)\n",
    "    \n",
    "    return Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = make_siamese_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SiameseNetwork\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_img (InputLayer)          [(None, 100, 100, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "validation_img (InputLayer)     [(None, 100, 100, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 64)           76812       input_img[0][0]                  \n",
      "                                                                 validation_img[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "distance (L1Dist)               (None, 64)           0           sequential[0][0]                 \n",
      "                                                                 sequential[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            65          distance[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 76,877\n",
      "Trainable params: 76,829\n",
      "Non-trainable params: 48\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cross_loss = tf.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(1e-4) # 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(batch):\n",
    "    \n",
    "    # Record all of our operations\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Get anchor and positive/negative image\n",
    "        X = batch[:2]\n",
    "        \n",
    "        # Get label\n",
    "        #if batch[2] == 1:\n",
    "            \n",
    "        \n",
    "        y = batch[2]\n",
    "        \n",
    "        # Forward pass\n",
    "        yhat = siamese_model(X, training=True)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = binary_cross_loss(y, yhat)\n",
    "#         loss = mse(y, yhat)\n",
    "        \n",
    "        \n",
    "    # Calculate gradients\n",
    "    grad = tape.gradient(loss, siamese_model.trainable_variables)\n",
    "    \n",
    "    # Calculate updated weights and apply to siamese model\n",
    "    opt.apply_gradients(zip(grad, siamese_model.trainable_variables))\n",
    "    \n",
    "    # Return loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(batch):\n",
    "    \n",
    "    # Record all of our operations\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        # Get anchor and positive/negative image\n",
    "        X = batch[:2]\n",
    "        \n",
    "        # Get label\n",
    "        y = batch[2]\n",
    "        \n",
    "        # Forward pass\n",
    "        yhat = siamese_model(X, training=True)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = binary_cross_loss(y, yhat)\n",
    "#         loss = mse(y, yhat)\n",
    "    \n",
    "    # Return loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data, test_data, EPOCHS):\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    epochs = []\n",
    "    \n",
    "    # Loop through epochs\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        print('\\n EPOCH {}/{}'.format(epoch, EPOCHS))\n",
    "        progbar = tf.keras.utils.Progbar(len(train_data) + len(test_data))\n",
    "        \n",
    "        # Loop through each batch\n",
    "        train_loss = 0\n",
    "        test_loss = 0\n",
    "        for idx, batch in enumerate(train_data):\n",
    "            # Run train step here\n",
    "            train_loss += train_step(batch)\n",
    "            test_loss += test_step(batch)\n",
    "            progbar.update(idx+1)\n",
    "            \n",
    "        train_loss = train_loss.numpy()\n",
    "        train_loss /= len(train_data)\n",
    "        test_loss = test_loss.numpy()\n",
    "        test_loss /= len(test_data)\n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        epochs.append(epoch)\n",
    "        print(\"\\n\")\n",
    "        print(\"training loss: \", train_loss)\n",
    "        print(\"testing loss: \", test_loss)\n",
    "    \n",
    "    # Save checkpoints\n",
    "    if epoch % 10 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    \n",
    "    return train_losses, test_losses, epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " EPOCH 1/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.7075756720776828\n",
      "testing loss:  1.6187127154806387\n",
      "\n",
      " EPOCH 2/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.6903614548017394\n",
      "testing loss:  1.5833431741465693\n",
      "\n",
      " EPOCH 3/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.6749488542664726\n",
      "testing loss:  1.5478286743164062\n",
      "\n",
      " EPOCH 4/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.6599526315365197\n",
      "testing loss:  1.5132690097974695\n",
      "\n",
      " EPOCH 5/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.6480774789486291\n",
      "testing loss:  1.4861566294794497\n",
      "\n",
      " EPOCH 6/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.6325716702443249\n",
      "testing loss:  1.4506524127462637\n",
      "\n",
      " EPOCH 7/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.6263347121904481\n",
      "testing loss:  1.435883729354195\n",
      "\n",
      " EPOCH 8/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.6085339672160599\n",
      "testing loss:  1.3950886933699898\n",
      "\n",
      " EPOCH 9/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.6016290052881781\n",
      "testing loss:  1.37915213211723\n",
      "\n",
      " EPOCH 10/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.5869524973743366\n",
      "testing loss:  1.3451536427373472\n",
      "\n",
      " EPOCH 11/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.582090269844487\n",
      "testing loss:  1.3338929881220278\n",
      "\n",
      " EPOCH 12/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.5647320657406213\n",
      "testing loss:  1.294013728266177\n",
      "\n",
      " EPOCH 13/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.5508991457381338\n",
      "testing loss:  1.2621616695238196\n",
      "\n",
      " EPOCH 14/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.5378503259622825\n",
      "testing loss:  1.231720385344132\n",
      "\n",
      " EPOCH 15/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.5263914432165757\n",
      "testing loss:  1.205334538998811\n",
      "\n",
      " EPOCH 16/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.5052698243339107\n",
      "testing loss:  1.1568256875743037\n",
      "\n",
      " EPOCH 17/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.499538169716889\n",
      "testing loss:  1.143879517264988\n",
      "\n",
      " EPOCH 18/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.48238048913343895\n",
      "testing loss:  1.1039735545282778\n",
      "\n",
      " EPOCH 19/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.4749652214770047\n",
      "testing loss:  1.0869488094163977\n",
      "\n",
      " EPOCH 20/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.46492922081137605\n",
      "testing loss:  1.063581549603006\n",
      "\n",
      " EPOCH 21/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.44632199125469857\n",
      "testing loss:  1.0208115785018257\n",
      "\n",
      " EPOCH 22/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.4382350489778339\n",
      "testing loss:  1.002312701681386\n",
      "\n",
      " EPOCH 23/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.42211604568193545\n",
      "testing loss:  0.9654953998068104\n",
      "\n",
      " EPOCH 24/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.4032204825923128\n",
      "testing loss:  0.9220576078995414\n",
      "\n",
      " EPOCH 25/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.39803447363511574\n",
      "testing loss:  0.9092617034912109\n",
      "\n",
      " EPOCH 26/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.377293568737102\n",
      "testing loss:  0.8624129088028617\n",
      "\n",
      " EPOCH 27/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.36691809600254277\n",
      "testing loss:  0.838527596515158\n",
      "\n",
      " EPOCH 28/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.3597813732219192\n",
      "testing loss:  0.8217066059941831\n",
      "\n",
      " EPOCH 29/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.35142229188163326\n",
      "testing loss:  0.802209356556768\n",
      "\n",
      " EPOCH 30/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.3352124196178508\n",
      "testing loss:  0.7652078711468241\n",
      "\n",
      " EPOCH 31/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.31786270861355764\n",
      "testing loss:  0.725582620371943\n",
      "\n",
      " EPOCH 32/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.3108461487968013\n",
      "testing loss:  0.7092631796131963\n",
      "\n",
      " EPOCH 33/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.294562447745845\n",
      "testing loss:  0.6725848239401112\n",
      "\n",
      " EPOCH 34/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.28651043154158684\n",
      "testing loss:  0.653735990109651\n",
      "\n",
      " EPOCH 35/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.2763029494375553\n",
      "testing loss:  0.6306290419205375\n",
      "\n",
      " EPOCH 36/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.26215756614253205\n",
      "testing loss:  0.5981700731360394\n",
      "\n",
      " EPOCH 37/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.24972690726226232\n",
      "testing loss:  0.5697229634160581\n",
      "\n",
      " EPOCH 38/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.23947690567880306\n",
      "testing loss:  0.5457006122754968\n",
      "\n",
      " EPOCH 39/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.2318004752105137\n",
      "testing loss:  0.5283600765725841\n",
      "\n",
      " EPOCH 40/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.22399342734858674\n",
      "testing loss:  0.5105565527211064\n",
      "\n",
      " EPOCH 41/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.2159674122648419\n",
      "testing loss:  0.4919956870701002\n",
      "\n",
      " EPOCH 42/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.2073935202832492\n",
      "testing loss:  0.4724650590316109\n",
      "\n",
      " EPOCH 43/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.20309135149110039\n",
      "testing loss:  0.4623979900194251\n",
      "\n",
      " EPOCH 44/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.18702911880781065\n",
      "testing loss:  0.4261958495430324\n",
      "\n",
      " EPOCH 45/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.19172681052729768\n",
      "testing loss:  0.436052944349206\n",
      "\n",
      " EPOCH 46/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.17980195891182377\n",
      "testing loss:  0.40927517932394275\n",
      "\n",
      " EPOCH 47/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.16576720183750368\n",
      "testing loss:  0.377250339673913\n",
      "\n",
      " EPOCH 48/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.16036474479819243\n",
      "testing loss:  0.36490510857623554\n",
      "\n",
      " EPOCH 49/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.15181316519683263\n",
      "testing loss:  0.3455700045046599\n",
      "\n",
      " EPOCH 50/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.1480183331471569\n",
      "testing loss:  0.3367802578469981\n",
      "\n",
      " EPOCH 51/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.14174948098524562\n",
      "testing loss:  0.32239766742872156\n",
      "\n",
      " EPOCH 52/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.13660425510046617\n",
      "testing loss:  0.310711902120839\n",
      "\n",
      " EPOCH 53/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.12841666419551057\n",
      "testing loss:  0.2920998075734014\n",
      "\n",
      " EPOCH 54/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.13007753300216962\n",
      "testing loss:  0.29563489167586615\n",
      "\n",
      " EPOCH 55/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.12197618664435621\n",
      "testing loss:  0.27721825889919116\n",
      "\n",
      " EPOCH 56/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.11712163349367538\n",
      "testing loss:  0.26645817963973334\n",
      "\n",
      " EPOCH 57/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.11257596285838001\n",
      "testing loss:  0.2559785635575004\n",
      "\n",
      " EPOCH 58/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.10777317802861051\n",
      "testing loss:  0.2448895495870839\n",
      "\n",
      " EPOCH 59/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.10638094848057009\n",
      "testing loss:  0.24166982070259427\n",
      "\n",
      " EPOCH 60/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.09834019642955852\n",
      "testing loss:  0.22351157146951425\n",
      "\n",
      " EPOCH 61/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.0950167494000129\n",
      "testing loss:  0.2158548106317935\n",
      "\n",
      " EPOCH 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.09694974827316571\n",
      "testing loss:  0.2199730458466903\n",
      "\n",
      " EPOCH 63/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.08644718494055406\n",
      "testing loss:  0.19655150952546493\n",
      "\n",
      " EPOCH 64/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.08143470872123286\n",
      "testing loss:  0.18467322639797046\n",
      "\n",
      " EPOCH 65/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.07846841272318138\n",
      "testing loss:  0.17833006900289786\n",
      "\n",
      " EPOCH 66/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.07992196532915223\n",
      "testing loss:  0.18124240377674933\n",
      "\n",
      " EPOCH 67/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.07296646765942844\n",
      "testing loss:  0.1654345989227295\n",
      "\n",
      " EPOCH 68/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.0713114378587255\n",
      "testing loss:  0.16153804115627124\n",
      "\n",
      " EPOCH 69/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.07047215047872292\n",
      "testing loss:  0.15969593628593112\n",
      "\n",
      " EPOCH 70/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.06654640863526542\n",
      "testing loss:  0.15098393481710684\n",
      "\n",
      " EPOCH 71/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.0648003119342732\n",
      "testing loss:  0.14693645809007727\n",
      "\n",
      " EPOCH 72/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.06405413825556917\n",
      "testing loss:  0.14505473427150561\n",
      "\n",
      " EPOCH 73/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.057446502289682067\n",
      "testing loss:  0.13046831670014755\n",
      "\n",
      " EPOCH 74/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.05722480450036391\n",
      "testing loss:  0.12981609676195227\n",
      "\n",
      " EPOCH 75/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.05506732778729133\n",
      "testing loss:  0.12492851589037024\n",
      "\n",
      " EPOCH 76/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.0531464972586002\n",
      "testing loss:  0.12055392887281335\n",
      "\n",
      " EPOCH 77/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.05360217814175588\n",
      "testing loss:  0.12132543066273564\n",
      "\n",
      " EPOCH 78/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.048701142365077756\n",
      "testing loss:  0.11029667439668076\n",
      "\n",
      " EPOCH 79/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.048012967379588004\n",
      "testing loss:  0.10896525175675102\n",
      "\n",
      " EPOCH 80/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.04673091420587504\n",
      "testing loss:  0.10601056140402089\n",
      "\n",
      " EPOCH 81/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.04590078569808096\n",
      "testing loss:  0.10404089222783627\n",
      "\n",
      " EPOCH 82/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.04139452610375746\n",
      "testing loss:  0.0939227705416472\n",
      "\n",
      " EPOCH 83/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.041278686163560396\n",
      "testing loss:  0.09345224629277768\n",
      "\n",
      " EPOCH 84/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.03760425999479474\n",
      "testing loss:  0.08536490150119948\n",
      "\n",
      " EPOCH 85/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.039222420386548315\n",
      "testing loss:  0.08872954741768214\n",
      "\n",
      " EPOCH 86/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.03588799035774087\n",
      "testing loss:  0.08140846957331119\n",
      "\n",
      " EPOCH 87/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.03578024315384199\n",
      "testing loss:  0.08106284037880275\n",
      "\n",
      " EPOCH 88/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.03602201983613788\n",
      "testing loss:  0.08107498417729916\n",
      "\n",
      " EPOCH 89/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.034643402639425024\n",
      "testing loss:  0.07845842319986095\n",
      "\n",
      " EPOCH 90/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.03139633502600328\n",
      "testing loss:  0.07093690270962923\n",
      "\n",
      " EPOCH 91/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.030793405928701726\n",
      "testing loss:  0.06968883846117102\n",
      "\n",
      " EPOCH 92/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.028360553507534962\n",
      "testing loss:  0.06432248716769011\n",
      "\n",
      " EPOCH 93/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.028424044824996084\n",
      "testing loss:  0.06433209129001784\n",
      "\n",
      " EPOCH 94/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.02893727680422225\n",
      "testing loss:  0.06495834951815398\n",
      "\n",
      " EPOCH 95/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.027377009391784668\n",
      "testing loss:  0.06193795411483101\n",
      "\n",
      " EPOCH 96/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.02402553468380334\n",
      "testing loss:  0.05459823297417682\n",
      "\n",
      " EPOCH 97/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.024657393401523807\n",
      "testing loss:  0.05586841832036558\n",
      "\n",
      " EPOCH 98/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.02304173415561892\n",
      "testing loss:  0.05234051269033681\n",
      "\n",
      " EPOCH 99/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.024885166366145295\n",
      "testing loss:  0.0561406249585359\n",
      "\n",
      " EPOCH 100/100\n",
      "53/76 [===================>..........] - ETA: 3s\n",
      "\n",
      "training loss:  0.022392333678479464\n",
      "testing loss:  0.05066339347673499\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'checkpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-1ea790f637b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-7e69a15f7ce7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_data, test_data, EPOCHS)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Save checkpoints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'checkpoint' is not defined"
     ]
    }
   ],
   "source": [
    "train_losses, test_losses, epochs = train(train_data, test_data, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_loss(train_losses, test_losses, epochs):\n",
    "    plt.plot(epochs, train_losses)\n",
    "    plt.plot(epochs, test_losses)\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_loss(train_losses, test_losses, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of test data\n",
    "test_input, test_val, y_true = test_data.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = siamese_model.predict([test_input, test_val])\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post procrssing the results\n",
    "[1 if prediction > 0.5 else 0 for prediction in y_hat ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot size\n",
    "plt.figure(figsize=(18,8))\n",
    "\n",
    "# Set first subplot\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(test_input[0][:,:,0],cmap='gray')\n",
    "\n",
    "# Set second subplot\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(test_val[0][:,:,0],cmap='gray')\n",
    "\n",
    "\n",
    "# Render cleanly\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model_filename = 'siamesemodel_v15.h5'\n",
    "tflite_filename = 'siamesemodel_v15_lite.tflite'\n",
    "tflite_model_name = 'siamesemodel_v15_lite'\n",
    "c_model_name = 'siamesemodel_v15_lite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "siamese_model.save(keras_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model\n",
    "model = tf.keras.models.load_model(keras_model_filename,\\\n",
    "                                  custom_objects={'L1Dist':L1Dist, 'MSE':tf.losses.MeanSquaredError})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with reloaded model\n",
    "model.predict([test_input, test_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "model = tf.keras.models.load_model(keras_model_filename,\\\n",
    "                                  custom_objects={'L1Dist':L1Dist, 'MSE':tf.losses.MeanSquaredError})\n",
    "converter = lite.TFLiteConverter.from_keras_model(model)\n",
    "converted_model = converter.convert()\n",
    "\n",
    "generated_dir = pathlib.Path(\"generated/\")\n",
    "generated_dir.mkdir(exist_ok=True, parents=True)\n",
    "converted_model_file = generated_dir/tflite_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_model_file.write_bytes(converted_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
